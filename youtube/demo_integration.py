#!/usr/bin/env python3
"""
Demo de IntegraÃ§Ã£o: LLM + Servidor HTTP MCP

Este script demonstra como um cliente LLM pode descobrir e consumir
automaticamente as ferramentas disponÃ­veis em um servidor HTTP MCP.

Arquitetura:
1. Servidor HTTP (http_server.py) expÃµe ferramentas MCP via REST API
2. Cliente LLM descobre automaticamente as ferramentas disponÃ­veis
3. LLM usa as ferramentas para responder perguntas em linguagem natural
"""

import requests
import json
from langchain_openai import AzureChatOpenAI
from pydantic import SecretStr
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent
import os
from dotenv import load_dotenv
from typing import Dict, Any, List

load_dotenv()

# ConfiguraÃ§Ã£o do Azure OpenAI
azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
azure_api_key = os.getenv("AZURE_OPENAI_API_KEY")
azure_deployment = os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT_NAME") or os.getenv("AZURE_MODEL_NAME")
azure_api_version = os.getenv("OPENAI_API_VERSION")

# URL do servidor HTTP MCP
SERVER_URL = "http://localhost:8000"

class MCPHTTPClient:
    """Cliente que consome ferramentas MCP via HTTP"""
    
    def __init__(self, server_url: str):
        self.server_url = server_url
        self.discovered_tools = []
        self.server_info = {}
        
    def discover_server_capabilities(self) -> bool:
        """Descobre automaticamente as capacidades do servidor MCP"""
        print(f"ğŸ” Descobrindo capacidades do servidor: {self.server_url}")
        
        try:
            # 1. InformaÃ§Ãµes gerais da API
            response = requests.get(f"{self.server_url}/")
            if response.status_code == 200:
                self.server_info = response.json()
                print(f"âœ… Servidor: {self.server_info.get('message', 'Desconhecido')}")
                print(f"   VersÃ£o: {self.server_info.get('version', 'N/A')}")
            
            # 2. Descoberta de ferramentas MCP
            tools_response = requests.get(f"{self.server_url}/tools")
            if tools_response.status_code == 200:
                tools_data = tools_response.json()
                self.discovered_tools = tools_data.get('tools', [])
                
                print(f"\nğŸ› ï¸  Ferramentas MCP descobertas: {len(self.discovered_tools)}")
                for i, tool in enumerate(self.discovered_tools, 1):
                    name = tool.get('name', 'Sem nome')
                    desc = tool.get('description', 'Sem descriÃ§Ã£o')
                    print(f"   {i}. {name}: {desc}")
                    
                    # Mostra schema de entrada se disponÃ­vel
                    if 'inputSchema' in tool:
                        schema = tool['inputSchema']
                        if 'properties' in schema:
                            props = list(schema['properties'].keys())
                            print(f"      ParÃ¢metros: {', '.join(props)}")
                
                return True
            else:
                print(f"âŒ Erro ao descobrir ferramentas: {tools_response.status_code}")
                return False
                
        except requests.exceptions.ConnectionError:
            print(f"âŒ NÃ£o foi possÃ­vel conectar ao servidor: {self.server_url}")
            print("   Certifique-se de que o servidor HTTP estÃ¡ rodando")
            return False
        except Exception as e:
            print(f"âŒ Erro inesperado: {e}")
            return False
    
    def call_tool_by_name(self, tool_name: str, **params) -> Dict[str, Any]:
        """Chama uma ferramenta especÃ­fica pelo nome"""
        try:
            # Mapeia nomes de ferramentas para endpoints HTTP
            endpoint_mapping = {
                'add': '/add',
                'subtract': '/subtract'
            }
            
            endpoint = endpoint_mapping.get(tool_name)
            if not endpoint:
                return {
                    'success': False,
                    'error': f"Ferramenta '{tool_name}' nÃ£o mapeada para endpoint HTTP"
                }
            
            # Faz a requisiÃ§Ã£o HTTP
            response = requests.post(f"{self.server_url}{endpoint}", json=params)
            
            if response.status_code == 200:
                result = response.json()
                return {
                    'success': True,
                    'result': result.get('result'),
                    'tool_name': tool_name,
                    'params': params
                }
            else:
                return {
                    'success': False,
                    'error': f"HTTP {response.status_code}: {response.text}"
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': f"Erro na chamada: {str(e)}"
            }

# InstÃ¢ncia global do cliente MCP
mcp_client = MCPHTTPClient(SERVER_URL)

def create_llm_tools() -> List:
    """Cria ferramentas LangChain que consomem o servidor MCP via HTTP"""
    
    @tool
    def discover_available_tools() -> str:
        """Descobre e lista todas as ferramentas matemÃ¡ticas disponÃ­veis no servidor MCP."""
        if not mcp_client.discovered_tools:
            if not mcp_client.discover_server_capabilities():
                return "âŒ NÃ£o foi possÃ­vel descobrir ferramentas no servidor MCP"
        
        if not mcp_client.discovered_tools:
            return "âŒ Nenhuma ferramenta encontrada no servidor"
        
        tool_descriptions = []
        for tool in mcp_client.discovered_tools:
            name = tool.get('name', 'Sem nome')
            desc = tool.get('description', 'Sem descriÃ§Ã£o')
            
            # Extrai informaÃ§Ãµes do schema
            schema_info = ""
            if 'inputSchema' in tool and 'properties' in tool['inputSchema']:
                props = tool['inputSchema']['properties']
                param_list = []
                for param_name, param_info in props.items():
                    param_type = param_info.get('type', 'unknown')
                    param_desc = param_info.get('description', '')
                    param_list.append(f"{param_name} ({param_type}): {param_desc}")
                
                if param_list:
                    schema_info = f"\n    ParÃ¢metros: {'; '.join(param_list)}"
            
            tool_descriptions.append(f"â€¢ {name}: {desc}{schema_info}")
        
        return f"Ferramentas disponÃ­veis no servidor MCP:\n" + "\n".join(tool_descriptions)
    
    @tool
    def add_numbers(a: int, b: int) -> str:
        """Soma dois nÃºmeros inteiros usando o servidor MCP.
        
        Args:
            a: Primeiro nÃºmero
            b: Segundo nÃºmero
            
        Returns:
            Resultado da soma
        """
        result = mcp_client.call_tool_by_name('add', a=a, b=b)
        
        if result['success']:
            return f"âœ… {a} + {b} = {result['result']}"
        else:
            return f"âŒ Erro na soma: {result['error']}"
    
    @tool
    def subtract_numbers(a: int, b: int) -> str:
        """Subtrai dois nÃºmeros inteiros usando o servidor MCP.
        
        Args:
            a: Primeiro nÃºmero (minuendo)
            b: Segundo nÃºmero (subtraendo)
            
        Returns:
            Resultado da subtraÃ§Ã£o
        """
        result = mcp_client.call_tool_by_name('subtract', a=a, b=b)
        
        if result['success']:
            return f"âœ… {a} - {b} = {result['result']}"
        else:
            return f"âŒ Erro na subtraÃ§Ã£o: {result['error']}"
    
    return [discover_available_tools, add_numbers, subtract_numbers]

def setup_llm_agent():
    """Configura o agente LLM com as ferramentas MCP"""
    # ConfiguraÃ§Ã£o do modelo Azure OpenAI
    model = AzureChatOpenAI(
        azure_endpoint=azure_endpoint,
        azure_deployment=azure_deployment,
        api_key=SecretStr(azure_api_key) if azure_api_key else None,
        api_version=azure_api_version,
    )
    
    # Cria ferramentas que consomem o servidor MCP
    tools = create_llm_tools()
    
    # Cria agente React
    agent = create_react_agent(model, tools)
    
    return agent

def demo_automatic_discovery():
    """Demonstra a descoberta automÃ¡tica de ferramentas"""
    print("\n" + "="*60)
    print("ğŸš€ DEMO: Descoberta AutomÃ¡tica de Ferramentas MCP")
    print("="*60)
    
    # Tenta descobrir as ferramentas
    if mcp_client.discover_server_capabilities():
        print("\nâœ… Descoberta bem-sucedida!")
        
        # Testa chamadas diretas
        print("\nğŸ§ª Testando chamadas diretas Ã s ferramentas:")
        
        test_cases = [
            ('add', {'a': 15, 'b': 27}),
            ('subtract', {'a': 50, 'b': 18}),
            ('add', {'a': -5, 'b': 10}),
            ('subtract', {'a': 0, 'b': 5})
        ]
        
        for tool_name, params in test_cases:
            result = mcp_client.call_tool_by_name(tool_name, **params)
            if result['success']:
                a, b = params['a'], params['b']
                op = '+' if tool_name == 'add' else '-'
                print(f"   {a} {op} {b} = {result['result']}")
            else:
                print(f"   âŒ Erro em {tool_name}: {result['error']}")
        
        return True
    else:
        print("\nâŒ Falha na descoberta de ferramentas")
        return False

def demo_llm_integration():
    """Demonstra a integraÃ§Ã£o com LLM"""
    print("\n" + "="*60)
    print("ğŸ¤– DEMO: IntegraÃ§Ã£o LLM + Servidor MCP")
    print("="*60)
    
    try:
        agent = setup_llm_agent()
        
        # Casos de teste em linguagem natural
        test_queries = [
            "Quais ferramentas estÃ£o disponÃ­veis no servidor?",
            "Quanto Ã© 25 + 17?",
            "Subtraia 12 de 45",
            "Calcule 100 menos 37",
            "Some 999 com 1"
        ]
        
        print("\nğŸ—£ï¸  Testando perguntas em linguagem natural:")
        
        for i, query in enumerate(test_queries, 1):
            print(f"\n{i}. Pergunta: '{query}'")
            print("   Resposta: ", end="")
            
            try:
                response = agent.invoke({"messages": [query]})
                answer = response["messages"][-1].content
                print(answer)
            except Exception as e:
                print(f"âŒ Erro: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erro na configuraÃ§Ã£o do LLM: {e}")
        return False

def interactive_demo():
    """Demo interativo"""
    print("\n" + "="*60)
    print("ğŸ’¬ DEMO INTERATIVO: Converse com o LLM")
    print("="*60)
    print("\nO LLM pode usar automaticamente as ferramentas do servidor MCP!")
    print("\nExemplos de perguntas:")
    print("â€¢ 'Quanto Ã© 123 + 456?'")
    print("â€¢ 'Subtraia 25 de 100'")
    print("â€¢ 'Quais ferramentas vocÃª tem disponÃ­vel?'")
    print("â€¢ 'Liste as capacidades do servidor'")
    print("\nDigite 'sair' para encerrar.\n")
    
    try:
        agent = setup_llm_agent()
        
        while True:
            user_input = input("ğŸ§‘ VocÃª: ").strip()
            
            if user_input.lower() in ['sair', 'exit', 'quit', 'q']:
                print("ğŸ‘‹ Encerrando demo...")
                break
                
            if not user_input:
                continue
                
            print("ğŸ¤– LLM: ", end="")
            
            try:
                response = agent.invoke({"messages": [user_input]})
                answer = response["messages"][-1].content
                print(answer + "\n")
            except Exception as e:
                print(f"âŒ Erro: {e}\n")
                
    except Exception as e:
        print(f"âŒ Erro na configuraÃ§Ã£o: {e}")

def main():
    """FunÃ§Ã£o principal da demonstraÃ§Ã£o"""
    print("ğŸ¯ DEMONSTRAÃ‡ÃƒO: LLM consumindo ferramentas MCP via HTTP")
    print("\nEsta demo mostra como:")
    print("1. O cliente descobre automaticamente as ferramentas do servidor")
    print("2. O LLM usa essas ferramentas para responder perguntas")
    print("3. Tudo acontece via HTTP, sem dependÃªncias diretas do MCP")
    
    # Verifica se o servidor estÃ¡ rodando
    try:
        response = requests.get(f"{SERVER_URL}/")
        if response.status_code != 200:
            print(f"\nâŒ Servidor nÃ£o estÃ¡ respondendo em {SERVER_URL}")
            print("Por favor, inicie o servidor HTTP:")
            print("python http_server.py")
            return
    except requests.exceptions.ConnectionError:
        print(f"\nâŒ NÃ£o foi possÃ­vel conectar ao servidor em {SERVER_URL}")
        print("Por favor, inicie o servidor HTTP:")
        print("python http_server.py")
        return
    
    print(f"\nâœ… Servidor HTTP detectado em {SERVER_URL}")
    
    # Menu de opÃ§Ãµes
    while True:
        print("\n" + "="*50)
        print("ğŸ“‹ MENU DE DEMONSTRAÃ‡Ã•ES")
        print("="*50)
        print("1. ğŸ” Descoberta automÃ¡tica de ferramentas")
        print("2. ğŸ¤– IntegraÃ§Ã£o LLM (casos de teste)")
        print("3. ğŸ’¬ Demo interativo (chat com LLM)")
        print("4. ğŸšª Sair")
        
        choice = input("\nEscolha uma opÃ§Ã£o (1-4): ").strip()
        
        if choice == "1":
            demo_automatic_discovery()
        elif choice == "2":
            if demo_automatic_discovery():  # Precisa descobrir primeiro
                demo_llm_integration()
        elif choice == "3":
            if demo_automatic_discovery():  # Precisa descobrir primeiro
                interactive_demo()
        elif choice == "4":
            print("ğŸ‘‹ Encerrando demonstraÃ§Ã£o...")
            break
        else:
            print("âŒ OpÃ§Ã£o invÃ¡lida. Tente novamente.")
        
        input("\nPressione Enter para continuar...")

if __name__ == "__main__":
    main()